# -*- coding: utf-8 -*-
"""Preprocesamiento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdJVUkk_uDR0TO3BJQtA-fNZdWHtTK72

# Data loading.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import seaborn as sns

url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00344/Activity%20recognition%20exp.zip'

# For zip extraction.
from urllib.request import urlopen
from io import BytesIO
from zipfile import ZipFile
import os 
    

def download_and_unzip(url, extract_to='.'):
    isdir = os.path.isdir(extract_to) 
    if(isdir != True):
        print("Downloading dataset...")
        http_response = urlopen(url)
        zipfile = ZipFile(BytesIO(http_response.read()))
        zipfile.extractall(path=extract_to)

# download_and_unzip(url, "./dataset")

"""Change "filepath" to where the csv file is stored on personal google drive."""

dir = '/content/drive/' # Mount personal google drive on /content/drive/ colab files. 
from google.colab import drive
drive.mount(dir)

# TODO: CONVERITR TIMESTAMPS A FORMATO DE FECHA(DATETIME COMPLETO)

# v_dataset = pd.read_csv("drive/MyDrive/CM/uci_dataset.txt", sep="\s+", header=None)
features = pd.read_csv("drive/MyDrive/CM/uci_features.txt", sep="\s+", header=None)[1]

dataset = pd.read_csv('drive/MyDrive/CM/reduced_real_time.csv')
# dataset = pd.read_csv("./dataset/Activity recognition exp/Phones_accelerometer.csv")

# Interarrivals are in the range 9-11 millliseconds.

dataset_sampled = dataset
v_dataset.columns = features

# dataset_sampled = dataset.sample(n=int(len(dataset)/3))

# dataset_sampled = dataset
# Convert timestamp to date.
# >>> datetime.fromtimestamp(time/1e9, tz=timezone.utc)

print(dataset)

print("Dataset: ")
# 1880 seconds passed, approx 150 samples per second per device.
print(len(dataset["realtime"].unique()))
print(len(dataset["Device"].unique())) # five devices
print(len(dataset["User"].unique())) # 1 User.
print(len(dataset["gt"].unique())) # 7 Activities

"""# Preprocessing

## Gravity removal.
"""

def remove_gravity(data):
  # TODO: HOW GRAVITY EXTRACTION WORKS.
  # IF IT STARTS IN 0, what's
  alpha = 0.8

  gravity = [0, 0, 0]
  accNoGravity = [[], [], []]

  for i in range(len(data)):
    x = data['x'][i]
    y = data['y'][i]
    z = data['z'][i]

    gravity[0] = alpha * gravity[0] + (1 - alpha) * x
    gravity[1] = alpha * gravity[1] + (1 - alpha) * y
    gravity[2] = alpha * gravity[2] + (1 - alpha) * z

    
    accNoGravity[0].append(x - gravity[0])
    accNoGravity[1].append(y - gravity[1]) 
    accNoGravity[2].append(z - gravity[2])

  return accNoGravity[0], accNoGravity[1], accNoGravity[2]

# Remove gravity to complete dataset.
x_nograv, y_nograv, z_nograv = remove_gravity(dataset_sampled)
dataset_sampled["x_nograv"] = x_nograv
dataset_sampled["y_nograv"] = y_nograv
dataset_sampled["z_nograv"] = z_nograv

fig_1, axs_1 = plt.subplots(2,3, figsize=(25,10))
axs_1[0,0].plot(dataset_sampled["realtime"], dataset_sampled["x"])
axs_1[0,1].plot(dataset_sampled["realtime"], dataset_sampled["y"])
axs_1[0,2].plot(dataset_sampled["realtime"], dataset_sampled["z"])
axs_1[1,0].plot(dataset_sampled["realtime"], dataset_sampled["x_nograv"])
axs_1[1,1].plot(dataset_sampled["realtime"], dataset_sampled["y_nograv"])
axs_1[1,2].plot(dataset_sampled["realtime"], dataset_sampled["z_nograv"])

"""## Segmentation."""

segment_size = 200
segment_init = 0

def segment_data(segment_size, data):
  # Extract features from each segment.
  # Each segment will be preprocesseed and features will be saved.
  # With an array segment_nmbr  is taken by index.
  # Not necesary to complete the last segment which may not have the 
  #   same stablished  size
  segments = {}
  segment_number = 0
  for i in range(0, len(data), segment_size):
    segment_init = i
    segment = data.iloc[i:i+segment_size]
    segments[segment_number] = {
            'init': segment_init,
            'size': len(segment),
            'data': segment
        }

    # Preprocess segment(Extract features)
    segment_number += 1

  return segments

segments = segment_data(segment_size, dataset_sampled)

"""## Features Extraction.

### Magnitude
"""

def get_magnitudes(data, nograv=False):
  magnitudes = []
  for i in range(len(data)):
    x = data["x_nograv" if nograv else "x"].iloc[i]
    y = data["y_nograv" if nograv else "y"].iloc[i]
    z = data["z_nograv" if nograv else "z"].iloc[i]

    magnitudes.append(math.sqrt((x ** 2) + (y ** 2) + (z ** 2)))

  return magnitudes

for k in segments:
  segments[k]['data']["magnitudes"] = get_magnitudes(segments[k]['data'], nograv=True)

segments[0]

"""### Features"""

def get_features(data):
  # Get mean, std, percentils, etc.
  return  {
      'x_mean': np.mean(data["x"]),
      'y_mean': np.mean(data["y"]),
      'z_mean': np.mean(data["z"]),
      'x_nograv_mean': np.mean(data["x_nograv"]),
      'y_nograv_mean': np.mean(data["y_nograv"]),
      'z_nograv_mean': np.mean(data["z_nograv"]),
      'm_mean': np.mean(data["magnitudes"]),
      'm_std': np.std(data["magnitudes"])
  }

features = {} 

for k in segments:
  features[k] = get_features(segments[k]['data'])

fts = pd.DataFrame(features).transpose()
print(fts)

"""# Comparison with UCI dataset."""

fig, axs = plt.subplots(3,3,figsize=(15,15))
axs[0,0].plot(fts["m_mean"], fts["m_std"])
axs[0,1].scatter(fts["m_mean"], fts["m_std"])
axs[0,2].hist(fts["m_mean"])
axs[0,2].set_title("magnitudes_means")
axs[1,0].hist(fts["x_mean"])
axs[0,2].set_title("x_means")
axs[1,1].hist(fts["y_mean"])
axs[1,2].hist(fts["z_mean"])
axs[2,0].hist(fts["x_nograv_mean"])
axs[2,1].hist(fts["y_nograv_mean"])
axs[2,2].hist(fts["z_nograv_mean"])

# plt.plot(v_dataset["m_mean"], fts["m_std"])

# print(segments)

all_magnitudes = []
for k in segments:
  all_magnitudes.append(segments[k]['data']['magnitudes'])

# print(all_magnitudes)

fig_2, axs_2 = plt.subplots(1,2,figsize=(10,5))

axs_2[0].plot(fts["m_mean"], fts["m_std"])
axs_2[1].plot(v_dataset["tBodyAccMag-mean()"], v_dataset["tBodyAccMag-std()"])

fig_3, axs_3 = plt.subplots(3,2,figsize=(10,15))

axs_3[0,0].hist(fts["x_mean"])
axs_3[0,1].hist(v_dataset["tBodyAcc-mean()-X"])
axs_3[1,0].hist(fts["y_mean"])
axs_3[1,1].hist(v_dataset["tBodyAcc-mean()-Y"])
axs_3[2,0].hist(fts["z_mean"])
axs_3[2,1].hist(v_dataset["tBodyAcc-mean()-Z"])

"""# Test correlation"""

fts # Data per segment. number of rows is number of segments.

v_dataset

"""Get features globally, i.e. of all dataset.

#***TODO***
- [ ] Get features per class.
- [ ] Plot features per class on same scatter plot.
- [ ] Quntify results and compare between UCI and Heterogeneity.]
- [ ] Segment per class and compare.

## Clasification.

Distance classification as base algorithm(Pending implementation). Change afterwards.
"""

import numpy as np
import pandas as pd
import math

def get_eu_distances(means, norm_data, nc, test_data):
    distances = {}

    for c in range(1, nc+1):
        distances[c] = np.sqrt((test_data - means[c]).transpose().dot(test_data - means[c]))

    return distances

def get_means(norm_data, nc):
    mus = {} # Means
    ci = {} # Class Instances
    for c in range(1, nc+1):
        ci[c] = norm_data[norm_data["y"] == c]
        mus[c] = ci[c].drop('y', axis=1).mean()
    return mus

c_means = get_means(norm_data, nc) # Means per class
# distances = euclidean.get_distances(c_means, norm_data, nc, test_data[len(test_data)-1])
for i in range(len(norm_data)):
# print("Test vector: \n", test_data[i])
predicted = []
distances = get_eu_distances(c_means, norm_data, nc, test_data[i])
max = [*distances][0] # Default distance
for c in range(1, nc+1):
  if(distances[c] < distances[max]):
    max = c

predicted.append(max)