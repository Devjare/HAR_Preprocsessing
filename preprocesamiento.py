# -*- coding: utf-8 -*-
"""Preprocesamiento.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdJVUkk_uDR0TO3BJQtA-fNZdWHtTK72

# Data loading.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import seaborn as sns
from datetime import datetime, timezone
from methods import *

"""Change "filepath" to where the csv file is stored on personal google drive."""

# TODO: CONVERITR TIMESTAMPS A FORMATO DE FECHA(DATETIME COMPLETO)

path = "./dataset/reduced.csv"
dataset = pd.read_csv(path)
times = dataset["Arrival_Time"]

dataset = dataset.drop(["Creation_Time", "Arrival_Time"], axis=1)
# Interarrivals are in the range 9-11 millliseconds.

dataset_sampled = dataset

# dataset_sampled = dataset.head(len(dataset)/3)
# Convert timestamp to date.
for i in range(len(times)):
    times[i] = datetime.fromtimestamp(times[i]/1e9, tz=timezone.utc)

dataset_sampled["Realtime"] = times

print("Dataset: \n")
# print(dataset)

# 1880 seconds passed, approx 150 samples per second per device.
print(len(dataset["Realtime"].unique()))
print(len(dataset["Device"].unique())) # five devices
devices = dataset["Device"].unique()
print(len(dataset["User"].unique())) # 1 User.
print(len(dataset["gt"].unique())) # 7 Activities

"""# Preprocessing """

""" # Get data per device (Not relevant for classification yet)
    Merely for test description info.
"""

data_per_device = {}
for i in range(len(devices)):
    d = devices[i]
    data = dataset[dataset["Device"] == d] 
    data_per_device[d] = data
    # print("******* DEVICE: ", d)
    # print("Data: \n", data)
    seconds = len(data["Realtime"].unique())
    # print("Data sampled for %d seconds" % (seconds))

""" ## Gravity removal. """
# Remove gravity to complete dataset.
x_nograv, y_nograv, z_nograv = remove_gravity(dataset_sampled)
dataset_sampled["x_nograv"] = x_nograv
dataset_sampled["y_nograv"] = y_nograv
dataset_sampled["z_nograv"] = z_nograv

""" Plot difference with and without gravity """
# fig_1, axs_1 = plt.subplots(2,3, figsize=(25,10))
# axs_1[0,0].plot(dataset_sampled["realtime"], dataset_sampled["x"])
# axs_1[0,1].plot(dataset_sampled["realtime"], dataset_sampled["y"])
# axs_1[0,2].plot(dataset_sampled["realtime"], dataset_sampled["z"])
# axs_1[1,0].plot(dataset_sampled["realtime"], dataset_sampled["x_nograv"])
# axs_1[1,1].plot(dataset_sampled["realtime"], dataset_sampled["y_nograv"])
# axs_1[1,2].plot(dataset_sampled["realtime"], dataset_sampled["z_nograv"])

# """## Segmentation."""
# 
# From reference: 
#   A Comparative Study on Human Activity
#   Recognition Using Inertial Sensors in a Smartphone
sampling_freq = 200 # 200 samples per second.
window_size = 5 # 5 Seconds
segment_size = sampling_freq * window_size

segments = segment_data(segment_size, dataset_sampled)
print("Segments: ", len(segments))


"""## Features Extraction.

### Magnitude
"""

for k in segments:
  seg = segments[k]['data']
  seg["mag"] = get_magnitudes(seg, nograv=True)

print(segments[0])

"""### Features"""

features = {} 

for k in segments:
  features[k] = get_features(segments[k]['data'])

fts = pd.DataFrame(features).transpose()
print(fts)

# Each five seconds the predicted clas should be updated?
# or that predicted class must be updated each time data from the
# accelerometer is taken? i.e. update the class predicted, let's say
# that the sampling rate is 200Hz, them update 200 times per second.

# """# Test correlation"""
# 
# """Get features globally, i.e. of all dataset.
# 
# #***TODO***
# - [ ] Get features per class.
# - [ ] Plot features per class on same scatter plot.
# - [ ] Quntify results and compare between UCI and Heterogeneity.]
# - [ ] Segment per class and compare.
# 
# ## Clasification. """
